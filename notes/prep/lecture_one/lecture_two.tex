\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}

\newcommand{\ub}{\boldsymbol{u}}
\newcommand{\vb}{\boldsymbol{v}}

\begin{document}

\begin{center}
{\Large Lecture Two - Stanford CS231N}

\begin{tabular}{rl}
Name: & Eli Andrew
\end{tabular}
\end{center}

  \begin{itemize}
    \item \textbf{Image Classification}
    \begin{itemize}
        \item $L1$ Distance: $d_1(I_1, I_2) = \sum\limits_{p}|I^p_1 - I^p_2|$
        \item $L2$ Distance: $d_2(I_1, I_2) = \sqrt{\sum\limits_{p}(I^p_1 - I^p_2)^2}$
        \item $L2$ distance prefers many small differences to one big one (it's more forgiving)
        \item $L1$ and $L2$ are most common forms of the pnorm: $||x||_p = (|x_1|^p + \dotsb + |x_n|^p)^{\frac{1}{p}}$
    \end{itemize}
    \item \textbf{A Few Useful Things to Know about Machine Learning}
    \begin{itemize}
        \item Key criteria for selecting representation is which kinds of knowledge are easily expressed in it
        \item For example: if we know a lot about what makes examples similar then instance-based methods (SVM, kNN)
        would be a good choice. If we know about probabilistic dependencies then graphical models are a good fit (CRF).
        And if we know about preconditions for each field then IF \dots THEN rules may be best.
        \item Strong false assumptions (assuming independence) can be better than weak true ones, because the learner with
        the latter needs more data to avoid overfitting.
        \item Counter to the curse of dimensionality is the blessing of non-uniformity. This states that examples are not
        uniformly spread throughout the instance space, but rather on or near a low-dimensional manifold. For example: hand written  
        digit images make up a space much smaller than the space of all possible images of the same size.
        \item Features may be irrelevant in isolation but relevant in combination. For example: if the function is an XOR.
        \item Dumb algorithm with lots of data beats a clever algorithm with modest amounts.

    \end{itemize}
    \item \textbf{Linear Classification}
    \begin{itemize}
        \item Linear classifier form: $f(x_i, W, b) = Wx_i + b$
        \item Example:
        \begin{itemize}
          \item Training data: images $x_i \in R^D$ each associated with label $y_i$
          \item Here: $i = 1 \dotsc N$ and $y_i \in 1 \dotsc K$ so we have $N$ training examples
          and $K$ distinct categories (in CIFAR-10 $N=50,000$ and $K=10$)
          \item Each picture $x$ has it's pixels flattened out into size $[D, 1]$
          \item Weight (or parameter) matrix $W$ is of size $[K, D]$, and bias $b$ is $[K, 1]$
        \end{itemize}
        \item Important things to note:
        \begin{itemize}
          \item $W$ is evaluating all $K$ classifiers in parallel, where each row of $W$ corresponds
          to the classifier for that class (row 0 is classifier for class 0)
          \item $W, b$ are our the only things we can control (data $x_i$ is fixed)
        \end{itemize}
        \item Linear classifiers can be interpreted as template matching, where each classifier learns a
        template for its class and uses the inner product between the example and its class to determine its score
        \item The weights of the class can actually be plotted as a picture to view the template
        \item \textbf{Bias Trick}
        \begin{itemize}
          \item One way to simplify our classifier form is to absorb the bias into the $Wx_i$ term
          \item This can be done by (1) adding the bias term as another column in our $W$ so that it
          is now $[K, D+1]$, and then (2) adding a one value row to the end of the $x_i$ to make it $[D+1, 1]$
          \item Now, we have $f(x_i, W) = Wx_i$ where we still end up with the shape: $[K, 1]$ but we don't have
          the extra bias term
        \end{itemize}
        \item Image data preprocessing: it is important to both (1) center your data by subtracting the mean image from each
        image, and (2) scale each feature so that it ranges from [-1, 1]
    \end{itemize}
    \item \textbf{Loss functions and Other Classifiers:}
    \begin{itemize}
      \item \textbf{Multiclass SVM}:
      \begin{itemize}
        \item SVM loss is setup in a way that the classifier ``wants'' \textbf{the correct class for each image to have a score higher than
        the incorrect classes by some fixed margin $\Delta$}
        \item For each $x_i$ (flattened image pixels) and $y_i$ (correct class for image), we compute $s = f(x_i, W)$ where $s_j$ is the
        $j^{th}$ entry of $s$
        \item The multiclass SVM loss for the $i^{th}$ example is then: 
        \begin{gather*}
          L_i = \sum\limits_{j \neq y_i}max(0, s_j - s_{y_i} + \Delta)
        \end{gather*}
      \end{itemize}
      \item \textbf{Regularization:}
      \begin{itemize}
        \item Any classifier parameters $W$ that correctly classify a set of points can be duplicated by replacing
        it with $\lambda W$ where $\lambda > 1$ (since it will uniformly stretch the loss)
        \item We want to have a preference for one particular set of $W$
        \item To do this we use a Regularization Penalty: 
        \begin{gather*}
          R(W) = \sum\limits_k\sum\limits_l W_{k, l}^2
        \end{gather*}
        \item So, full loss function becomes:
        \begin{gather*}
          L = \frac{1}{N}\sum\limits_i L_i + \lambda R(W)
        \end{gather*}
      \end{itemize}
    \end{itemize}
    \item \textbf{Softmax Classifier}:
    \begin{itemize}
      \item Generalization of the logistic regression classifier to multi-class
    \end{itemize}
  \end{itemize}


\end{document}
